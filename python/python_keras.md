# üß† Cheatsheet de Keras

Este cheatsheet cubre los aspectos esenciales de Keras, la API de alto nivel de TensorFlow para construir y entrenar modelos de aprendizaje profundo.

## üîç Conceptos B√°sicos

*   **Modelo (Model):** üß† Contenedor que define la arquitectura de la red neuronal (capas, conexiones, etc.).
*   **Capa (Layer):** üß± Bloque de construcci√≥n fundamental de un modelo.  Realiza una transformaci√≥n (lineal o no lineal) sobre los datos de entrada.
*   **Funci√≥n de Activaci√≥n (Activation Function):** ‚ö°Ô∏è Funci√≥n no lineal aplicada a la salida de una capa (ReLU, sigmoid, tanh, etc.).  Introduce no linealidad en el modelo.
*   **Optimizador (Optimizer):** üöÄ Algoritmo que ajusta los pesos del modelo durante el entrenamiento (Adam, SGD, RMSprop, etc.).
*   **Funci√≥n de P√©rdida (Loss Function):** üìâ Mide la diferencia entre las predicciones del modelo y los valores reales (categorical_crossentropy, mse, etc.).
*   **M√©trica (Metric):** üìä Mide el rendimiento del modelo (accuracy, precision, recall, etc.).
*   **Entrenamiento (Training):** üí™ Proceso iterativo de ajustar los pesos del modelo para minimizar la funci√≥n de p√©rdida.
*   **√âpoca (Epoch):** üîÑ Un ciclo completo de entrenamiento sobre todos los datos de entrenamiento.
*   **Lote (Batch):** üì¶ Subconjunto de datos de entrenamiento utilizado en una iteraci√≥n del optimizador.
*   **Validaci√≥n (Validation):** üß™ Evaluaci√≥n del modelo en un conjunto de datos separado (no utilizado para el entrenamiento) para monitorizar el sobreajuste (overfitting).
*   **Sobreajuste (Overfitting):** üìâ Cuando el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos.
*   **Regularizaci√≥n (Regularization):** üõ°Ô∏è T√©cnicas para prevenir el sobreajuste (dropout, L1/L2 regularization, etc.).
*   **Callback:** üìû Funci√≥n que se ejecuta en diferentes puntos durante el entrenamiento (por ejemplo, para guardar el modelo, detener el entrenamiento anticipadamente, etc.).
* **Transfer Learning**: ‚ôªÔ∏è Aprovechar un modelo pre-entrenado en un dataset grande.

## üöÄ Construcci√≥n de Modelos

Keras ofrece tres formas principales de construir modelos:

1.  **Modelo Secuencial (Sequential):**

    ```python
    from tensorflow import keras
    from tensorflow.keras import layers

    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(784,)),  # Capa de entrada
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')  # Capa de salida (10 clases)
    ])
    ```
    *   La forma m√°s sencilla.  Ideal para modelos con una secuencia lineal de capas.

2.  **API Funcional (Functional API):**

    ```python
    from tensorflow import keras
    from tensorflow.keras import layers, Model

    inputs = keras.Input(shape=(784,))
    x = layers.Dense(64, activation='relu')(inputs)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(10, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    ```
    *   M√°s flexible.  Permite crear modelos con m√∫ltiples entradas/salidas, capas compartidas, conexiones no lineales, etc.

3.  **Subclasificaci√≥n de Modelos (Model Subclassing):**

    ```python
    from tensorflow import keras
    from tensorflow.keras import layers, Model

    class MiModelo(Model):
        def __init__(self):
            super(MiModelo, self).__init__()
            self.dense1 = layers.Dense(64, activation='relu')
            self.dense2 = layers.Dense(64, activation='relu')
            self.dense3 = layers.Dense(10, activation='softmax')

        def call(self, inputs):
            x = self.dense1(inputs)
            x = self.dense2(x)
            return self.dense3(x)

    model = MiModelo()
    ```
    *   M√°xima flexibilidad.  Permite definir la l√≥gica de la propagaci√≥n hacia adelante (forward pass) de forma personalizada.  √ötil para modelos con flujos de control complejos.  *Pero* es m√°s dif√≠cil de usar, depurar y guardar/cargar.

## üß± Capas (Layers)

*   **`Dense`:** Capa densa (totalmente conectada).  `units`: n√∫mero de neuronas.  `activation`: funci√≥n de activaci√≥n.
*   **`Conv2D`:** Capa convolucional 2D (para im√°genes).  `filters`: n√∫mero de filtros.  `kernel_size`: tama√±o del filtro.  `strides`: paso de la convoluci√≥n.  `padding`: 'valid' o 'same'.
*   **`MaxPooling2D`:** Capa de max pooling 2D.  `pool_size`: tama√±o de la ventana de pooling.  `strides`: paso del pooling.
*   **`Dropout`:** Capa de dropout (regularizaci√≥n).  `rate`: fracci√≥n de neuronas a desactivar.
*   **`Flatten`:** Aplana la entrada (convierte un tensor multidimensional en un vector).
*   **`Embedding`:** Capa de embedding (para texto/secuencias).  `input_dim`: tama√±o del vocabulario.  `output_dim`: dimensi√≥n del embedding.
*   **`LSTM`:** Capa LSTM (red recurrente).  `units`: n√∫mero de unidades LSTM.
*   **`GRU`:** Capa GRU (otra red recurrente).
*   **`BatchNormalization`:** Normalizaci√≥n por lotes.
*   **`Activation`:** Aplica una funci√≥n de activaci√≥n.
*   **`Reshape`**: Cambia la forma de la entrada.
*   **`Input`**: Capa de entrada (se usa en la API funcional).

## ‚ö°Ô∏è Funciones de Activaci√≥n (Activation Functions)

*   **`relu`:** Rectified Linear Unit (la m√°s com√∫n).
*   **`sigmoid`:** Sigmoide (para clasificaci√≥n binaria).
*   **`softmax`:** Softmax (para clasificaci√≥n multiclase).
*   **`tanh`:** Tangente hiperb√≥lica.
*   **`elu`:** Exponential Linear Unit.
*   **`selu`:** Scaled Exponential Linear Unit.
*   **`linear`:** Identidad (sin activaci√≥n).

## üöÄ Optimizadores (Optimizers)

*   **`Adam`:** Adam (generalmente la mejor opci√≥n por defecto).
*   **`SGD`:** Descenso del gradiente estoc√°stico (con o sin momentum).
*   **`RMSprop`:** RMSprop.
*   **`Adagrad`:** Adagrad.
*   **`AdamW`**: Adam con weight decay.

```python
# Ejemplo de uso
optimizer = keras.optimizers.Adam(learning_rate=0.001)
```

## üìâ Funciones de P√©rdida (Loss Functions)

*   **`categorical_crossentropy`:** Para clasificaci√≥n multiclase (etiquetas one-hot).
*   **`sparse_categorical_crossentropy`:** Para clasificaci√≥n multiclase (etiquetas enteras).
*   **`binary_crossentropy`:** Para clasificaci√≥n binaria.
*   **`mse`:** Mean Squared Error (para regresi√≥n).
*   **`mae`:** Mean Absolute Error (para regresi√≥n).
*   **`huber`:** P√©rdida de Huber (robusta a outliers).

## üìä M√©tricas (Metrics)

*   **`accuracy`:** Precisi√≥n (para clasificaci√≥n).
*   **`categorical_accuracy`:** Precisi√≥n categ√≥rica.
*   **`sparse_categorical_accuracy`:** Precisi√≥n categ√≥rica (etiquetas enteras).
*   **`binary_accuracy`:** Precisi√≥n binaria.
*   **`precision`:** Precisi√≥n.
*   **`recall`:** Recall.
*   **`AUC`:** √Årea bajo la curva ROC.
*    `F1Score`: Combinaci√≥n de precision y recall

## ‚öôÔ∏è Compilaci√≥n del Modelo

```python
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```

*   `optimizer`: El optimizador a utilizar.
*   `loss`: La funci√≥n de p√©rdida.
*   `metrics`: Lista de m√©tricas para evaluar el modelo.

## üí™ Entrenamiento del Modelo

```python
history = model.fit(X_train, y_train,
                    batch_size=32,
                    epochs=10,
                    validation_data=(X_val, y_val),
                    callbacks=...) # Opcional: Lista de Callbacks
```

*   `X_train`, `y_train`: Datos de entrenamiento.
*   `batch_size`: Tama√±o del lote.
*   `epochs`: N√∫mero de √©pocas.
*   `validation_data`: Datos de validaci√≥n.
*   `callbacks`: Lista de callbacks.

## üß™ Evaluaci√≥n del Modelo

```python
loss, accuracy = model.evaluate(X_test, y_test)
```

*   `X_test`, `y_test`: Datos de prueba.

## üîÆ Predicci√≥n

```python
predictions = model.predict(X_new)
```

*   `X_new`: Nuevos datos para hacer predicciones.

## üìû Callbacks

*   **`ModelCheckpoint`:** Guarda el modelo peri√≥dicamente.
*   **`EarlyStopping`:** Detiene el entrenamiento anticipadamente si la m√©trica de validaci√≥n deja de mejorar.
*   **`TensorBoard`:** Para visualizaci√≥n con TensorBoard.
*   **`ReduceLROnPlateau`:** Reduce el learning rate si la m√©trica de validaci√≥n se estanca.
*   **`LearningRateScheduler`:** Permite programar el learning rate.
*  `CSVLogger`: Guarda los logs en un fichero CSV.
* `LambdaCallback`: Permite crear Callbacks customizados

```python
callbacks = [
    keras.callbacks.ModelCheckpoint(filepath='mi_modelo_{epoch}.h5', save_best_only=True, monitor='val_loss'),
    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),
    keras.callbacks.TensorBoard(log_dir='./logs')
]
```

## üíæ Guardar y Cargar Modelos

```python
# Guardar el modelo completo (arquitectura + pesos + estado del optimizador)
model.save('mi_modelo.h5')  # Formato HDF5
model.save('mi_modelo')     # Formato SavedModel (recomendado)

# Cargar el modelo
loaded_model = keras.models.load_model('mi_modelo.h5')  # O 'mi_modelo'
```

## ‚ôªÔ∏è Transfer Learning (Aprovechar Modelos Pre-entrenados)

```python
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, Model

# Cargar un modelo pre-entrenado (sin la capa de clasificaci√≥n)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Congelar las capas del modelo base (para no entrenarlas)
base_model.trainable = False

# A√±adir capas personalizadas encima
inputs = keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)  # training=False es importante
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(10, activation='softmax')(x)  # Capa de salida para 10 clases
model = Model(inputs, x)
model.compile(...)
#Ahora se puede entrenar, pero solo se entrenar√°n las √∫ltimas capas.
```

## ‚ú® Consejos Adicionales

*   Comienza con modelos simples y aumenta gradualmente la complejidad.
*   Usa la API funcional para mayor flexibilidad.
*   Monitoriza el entrenamiento con TensorBoard.
*   Utiliza callbacks para guardar el modelo y detener el entrenamiento anticipadamente.
*   Experimenta con diferentes arquitecturas, optimizadores, funciones de p√©rdida y m√©tricas.
*   Considera el transfer learning para acelerar el entrenamiento y mejorar el rendimiento.
*   Consulta la documentaci√≥n oficial de Keras: [https://keras.io/](https://keras.io/)

Este cheatsheet de Keras proporciona una visi√≥n completa de los componentes y t√©cnicas clave, junto con ejemplos pr√°cticos. ¬°Espero que te sea muy √∫til!