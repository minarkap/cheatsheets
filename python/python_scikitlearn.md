#  Scikit-learn (sklearn) Cheatsheet: Machine Learning en Python üöÄ

## üåü **Conceptos Fundamentales**

*   **Scikit-learn:** Biblioteca de Python de c√≥digo abierto para *machine learning*.  Proporciona herramientas simples y eficientes para:
    *   Clasificaci√≥n.
    *   Regresi√≥n.
    *   Clustering.
    *   Reducci√≥n de dimensionalidad.
    *   Selecci√≥n de modelos.
    *   Preprocesamiento de datos.
*   **Estimadores (Estimators):**  Objetos que aprenden de los datos.  Tienen un m√©todo `fit()` para entrenar el modelo y un m√©todo `predict()` para hacer predicciones.  Ejemplos: `LinearRegression`, `LogisticRegression`, `SVC`, `KMeans`, etc.
*   **Transformadores (Transformers):**  Objetos que transforman los datos.  Tienen un m√©todo `fit()` para aprender los par√°metros de la transformaci√≥n y un m√©todo `transform()` para aplicar la transformaci√≥n.  Algunos tambi√©n tienen `fit_transform()`.  Ejemplos: `StandardScaler`, `MinMaxScaler`, `PCA`, etc.
*   **Pipelines:**  Encadenan varios estimadores y/o transformadores en un solo objeto.
*   **M√©tricas (Metrics):**  Funciones para evaluar el rendimiento de los modelos.
*   **Conjuntos de datos (Datasets):**  Scikit-learn incluye algunos conjuntos de datos de ejemplo (juguete) y funciones para cargar conjuntos de datos m√°s grandes.

## üíª **Instalaci√≥n**

```bash
pip install scikit-learn
```

## üîÑ **Flujo de Trabajo General (Workflow)**

1.  **Cargar los datos:**
2.  **Preprocesar los datos:**
    *   Limpieza (manejo de valores faltantes, etc.).
    *   Transformaci√≥n (escalado, codificaci√≥n de variables categ√≥ricas, etc.).
    *   Reducci√≥n de dimensionalidad (si es necesario).
3.  **Dividir los datos:** En conjuntos de entrenamiento (training), validaci√≥n (validation) y prueba (test).
4.  **Elegir un modelo:**
5.  **Entrenar el modelo:** Usar el m√©todo `fit()` con los datos de entrenamiento.
6.  **Evaluar el modelo:** Usar m√©tricas apropiadas (accuracy, precision, recall, F1-score, MSE, R¬≤, etc.) con los datos de validaci√≥n/prueba.
7.  **Ajustar los hiperpar√°metros:** Usar t√©cnicas como *grid search* o *randomized search* para encontrar los mejores hiperpar√°metros del modelo.
8.  **Hacer predicciones:** Usar el m√©todo `predict()` con nuevos datos.

## üìö **M√≥dulos Principales**

*   **`sklearn.preprocessing`:**  Preprocesamiento de datos.
*   **`sklearn.model_selection`:**  Selecci√≥n de modelos, validaci√≥n cruzada, ajuste de hiperpar√°metros.
*   **`sklearn.linear_model`:**  Modelos lineales (regresi√≥n lineal, regresi√≥n log√≠stica, etc.).
*   **`sklearn.svm`:**  Support Vector Machines (SVM).
*   **`sklearn.tree`:**  √Årboles de decisi√≥n.
*   **`sklearn.ensemble`:**  M√©todos de ensemble (Random Forest, Gradient Boosting, etc.).
*   **`sklearn.neighbors`:**  Vecinos m√°s cercanos (k-NN).
*   **`sklearn.naive_bayes`:**  Naive Bayes.
*   **`sklearn.cluster`:**  Clustering (K-Means, etc.).
*   **`sklearn.decomposition`:**  Reducci√≥n de dimensionalidad (PCA, etc.).
*   **`sklearn.metrics`:**  M√©tricas de evaluaci√≥n.
*   **`sklearn.pipeline`:**  Pipelines.
*   **`sklearn.datasets`:**  Conjuntos de datos de ejemplo.
*  **`sklearn.impute`**: Imputaci√≥n de valores perdidos.

## üíæ **Carga de Datos (Datasets)**

*   **Datasets de juguete (toy datasets):**
    *   `load_boston()` (regresi√≥n, *obsoleto*).
    *   `load_iris()` (clasificaci√≥n).
    *   `load_diabetes()` (regresi√≥n).
    *   `load_digits()` (clasificaci√≥n).
    *   `load_wine()` (clasificaci√≥n).
    *   `load_breast_cancer()` (clasificaci√≥n).

    ```python
    from sklearn.datasets import load_iris

    iris = load_iris()
    X = iris.data  # Caracter√≠sticas (features)
    y = iris.target  # Etiquetas (targets)
    feature_names = iris.feature_names
    ```

*   **Cargar datos desde archivos:**  Generalmente se usa pandas para cargar datos desde archivos CSV, Excel, etc.

    ```python
    import pandas as pd

    df = pd.read_csv("mi_archivo.csv")
    X = df[["columna1", "columna2"]]  # Seleccionar columnas de caracter√≠sticas
    y = df["etiqueta"]  # Seleccionar la columna de etiquetas
    ```
* **`sklearn.datasets.fetch_*`**: Funciones para descargar y cargar datasets m√°s grandes.

## üßπ **Preprocesamiento (`sklearn.preprocessing`)**

*   **Escalado:**
    *   **`StandardScaler`:**  Estandariza los datos (media 0, desviaci√≥n est√°ndar 1).
    *   **`MinMaxScaler`:**  Escala los datos a un rango (por defecto, [0, 1]).
    *   **`RobustScaler`:**  Escalado robusto a outliers (usa la mediana y el rango intercuart√≠lico).
    *   **`MaxAbsScaler`:**  Escala los datos dividiendo por el valor m√°ximo absoluto.
    *   **`Normalizer`:** Normaliza cada *muestra* (fila) a la unidad de norma (√∫til para texto).

    ```python
    from sklearn.preprocessing import StandardScaler

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)  # Ajusta y transforma
    # X_scaled = scaler.transform(X) # Si ya se ajust√≥ con otros datos

    ```

*   **Codificaci√≥n de variables categ√≥ricas:**
    *   **`OneHotEncoder`:**  Crea variables dummy (one-hot encoding).
    *   **`OrdinalEncoder`:**  Codifica variables categ√≥ricas como enteros (para variables ordinales).
    * **`LabelEncoder`**: Codifica las etiquetas *y* como enteros (generalmente no se usa para las *X*).

    ```python
    from sklearn.preprocessing import OneHotEncoder

    enc = OneHotEncoder(handle_unknown='ignore') # handle_unknown='ignore' para evitar errores con valores nuevos
    X_encoded = enc.fit_transform(X_categorical) # X_categorical: matriz con variables categ√≥ricas.

    ```

*   **Imputaci√≥n de valores faltantes:**
    *   **`SimpleImputer`:**  Imputa valores faltantes usando la media, la mediana, el valor m√°s frecuente o un valor constante.
    *   **`KNNImputer`:**  Imputa valores faltantes usando el algoritmo k-NN.
    *   **`IterativeImputer`:** Imputaci√≥n multivariante (usa un modelo para predecir los valores faltantes).

    ```python
    from sklearn.impute import SimpleImputer

    imputer = SimpleImputer(strategy="mean")  # Reemplaza los valores faltantes por la media
    X_imputed = imputer.fit_transform(X)

    ```
* **`PolynomialFeatures`**: Genera caracter√≠sticas polin√≥micas e interacciones.
* **`FunctionTransformer`**: Aplica una funci√≥n arbitraria a los datos.
*  **Discretizaci√≥n:**
    *   **`KBinsDiscretizer`:**  Convierte variables num√©ricas en variables categ√≥ricas (discretizaci√≥n).

## ‚úÇÔ∏è **Divisi√≥n de Datos (`sklearn.model_selection`)**

*   **`train_test_split`:**  Divide los datos en conjuntos de entrenamiento y prueba.

    ```python
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% entrenamiento, 20% prueba
    # stratify=y: Para mantener la proporci√≥n de clases en problemas de clasificaci√≥n.
    ```

* **Validaci√≥n cruzada (Cross-validation):**
    *   **`cross_val_score`:**  Eval√∫a un modelo usando validaci√≥n cruzada.

        ```python
        from sklearn.model_selection import cross_val_score
        from sklearn.linear_model import LogisticRegression

        model = LogisticRegression()
        scores = cross_val_score(model, X, y, cv=5)  # Validaci√≥n cruzada de 5 folds
        print(f"Accuracy: {scores.mean()} (+/- {scores.std() * 2})")

        ```

    *   **`KFold`:**  Iterador de validaci√≥n cruzada K-Fold.
    *   **`StratifiedKFold`:**  K-Fold estratificado (mantiene la proporci√≥n de clases en cada fold).
    *   **`ShuffleSplit`:**  Genera divisiones aleatorias de entrenamiento/prueba.
    *   **`LeaveOneOut`:**  Deja un ejemplo fuera (Leave-One-Out).
    *   **`GroupKFold`:**  K-Fold con grupos (para datos donde las muestras de un mismo grupo no deben estar en diferentes folds).

## ü§ñ **Modelos (Estimadores)**

### üìà **Regresi√≥n**

*   **`LinearRegression`:**  Regresi√≥n lineal ordinaria.
*   **`Ridge`:**  Regresi√≥n lineal con regularizaci√≥n L2 (ridge regression).
*   **`Lasso`:**  Regresi√≥n lineal con regularizaci√≥n L1 (lasso regression).
*   **`ElasticNet`:**  Regresi√≥n lineal con regularizaci√≥n L1 y L2 (elastic net).
*   **`LogisticRegression`:**  Regresi√≥n log√≠stica (para clasificaci√≥n, a pesar del nombre).
*   **`SGDRegressor`:**  Regresi√≥n lineal con descenso de gradiente estoc√°stico (SGD).
*  `SVR`: Support Vector Regressor.
*  `DecisionTreeRegressor`
* `RandomForestRegressor`, `GradientBoostingRegressor`, `HistGradientBoostingRegressor` (Ensembles)
* `KNeighborsRegressor`

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"Coeficientes: {model.coef_}")
print(f"Intercepto: {model.intercept_}")

```

### üè∑Ô∏è **Clasificaci√≥n**

*   **`LogisticRegression`:**  Regresi√≥n log√≠stica.
*   **`SVC` (Support Vector Classifier):**  M√°quina de vectores de soporte (SVM).
*   **`DecisionTreeClassifier`:**  √Årbol de decisi√≥n.
*   **`RandomForestClassifier`:**  Random Forest.
*   **`GradientBoostingClassifier`:**  Gradient Boosting.
*  `HistGradientBoostingClassifier`: Gradient Boosting basado en histogramas (m√°s r√°pido para datasets grandes).
*   **`KNeighborsClassifier`:**  k-NN.
*   **`GaussianNB`:**  Naive Bayes Gaussiano.
*   **`SGDClassifier`:**  Clasificador lineal con descenso de gradiente estoc√°stico (SGD).

```python
from sklearn.svm import SVC

model = SVC(kernel="rbf", C=1.0, gamma="scale")
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"Accuracy: {model.score(X_test, y_test)}")

```

### üå≥ **Clustering**

*   **`KMeans`:**  K-Means clustering.
*   **`MiniBatchKMeans`**: Versi√≥n m√°s r√°pida de KMeans (para datasets grandes).
*   **`DBSCAN`:**  DBSCAN clustering.
*   **`AgglomerativeClustering`:**  Clustering aglomerativo.
*   **`MeanShift`:** Mean Shift clustering.
*  `AffinityPropagation`
* `SpectralClustering`

```python
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, random_state=42)
model.fit(X) # KMeans no supervisado, no necesita y
labels = model.labels_
centroids = model.cluster_centers_

```

### üìâ **Reducci√≥n de Dimensionalidad**

*   **`PCA` (Principal Component Analysis):**  An√°lisis de componentes principales.
*   **`TruncatedSVD`:**  Descomposici√≥n en valores singulares truncada (SVD).  √ötil para matrices dispersas.
*   **`NMF` (Non-negative Matrix Factorization):**  Factorizaci√≥n de matrices no negativas.
* `FastICA`: Independent Component Analysis.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)  # Reduce a 2 dimensiones
X_reduced = pca.fit_transform(X)
print(f"Varianza explicada: {pca.explained_variance_ratio_}")
```

## üìè **M√©tricas de Evaluaci√≥n (`sklearn.metrics`)**

### Clasificaci√≥n

*   **`accuracy_score`:**  Exactitud (accuracy).
*   **`precision_score`:**  Precisi√≥n.
*   **`recall_score`:**  Recall (sensibilidad).
*   **`f1_score`:**  F1-score (media arm√≥nica de precisi√≥n y recall).
*   **`roc_auc_score`:**  √Årea bajo la curva ROC (AUC).
*   **`confusion_matrix`:**  Matriz de confusi√≥n.
*   **`classification_report`:**  Informe de clasificaci√≥n (precision, recall, F1-score, support).
*  `roc_curve`: Calcula la curva ROC.
*  `average_precision_score`:  Calcula el √°rea bajo la curva Precision-Recall.
*  `log_loss`: P√©rdida logar√≠tmica (log loss).

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Precision: {precision_score(y_test, y_pred)}")
print(f"Recall: {recall_score(y_test, y_pred)}")
print(f"F1-score: {f1_score(y_test, y_pred)}")
print(f"Matriz de confusi√≥n:\n{confusion_matrix(y_test, y_pred)}")

```

### Regresi√≥n

*   **`mean_squared_error` (MSE):**  Error cuadr√°tico medio.
*   **`mean_absolute_error` (MAE):**  Error absoluto medio.
*   **`r2_score`:**  R¬≤ (coeficiente de determinaci√≥n).
*   **`explained_variance_score`:**  Varianza explicada.
* `max_error`: Error m√°ximo.
* `mean_squared_log_error`: Error cuadr√°tico logar√≠tmico medio.

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print(f"MSE: {mean_squared_error(y_test, y_pred)}")
print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
print(f"R¬≤: {r2_score(y_test, y_pred)}")

```
### Clustering

* `adjusted_rand_score`: Adjusted Rand Index.
* `silhouette_score`: Silhouette Coefficient.
* `completeness_score`, `homogeneity_score`, `v_measure_score`: Homogeneidad, completitud y V-measure.

## ‚öôÔ∏è **Ajuste de Hiperpar√°metros (`sklearn.model_selection`)**

*   **`GridSearchCV`:**  B√∫squeda exhaustiva en una cuadr√≠cula de hiperpar√°metros.
*   **`RandomizedSearchCV`:**  B√∫squeda aleatoria de hiperpar√°metros.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

param_grid = {
    "C": [0.1, 1, 10],
    "kernel": ["linear", "rbf"],
    "gamma": ["scale", "auto"]
}

model = SVC()
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Mejores par√°metros: {grid_search.best_params_}")
print(f"Mejor puntuaci√≥n: {grid_search.best_score_}")
best_model = grid_search.best_estimator_ # Mejor modelo

```

## ‚õìÔ∏è **Pipelines (`sklearn.pipeline`)**

*   **`Pipeline`:**  Encadena varios pasos (transformadores y un estimador final).

    ```python
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression

    pipeline = Pipeline([
        ("scaler", StandardScaler()),  # Paso 1: Escalado
        ("model", LogisticRegression())  # Paso 2: Regresi√≥n log√≠stica
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    ```

*   **`make_pipeline`:**  Funci√≥n para crear pipelines de forma m√°s concisa (no es necesario nombrar los pasos).

* **`FeatureUnion`**: Combina los resultados de varios *transformadores*.

## ‚ûï **Otros M√≥dulos y Funciones √ötiles**

*   **`sklearn.feature_selection`:**  Selecci√≥n de caracter√≠sticas.
    *   `SelectKBest`, `SelectPercentile`:  Seleccionan las mejores caracter√≠sticas seg√∫n una m√©trica.
    *   `RFE` (Recursive Feature Elimination):  Eliminaci√≥n recursiva de caracter√≠sticas.
    *   `VarianceThreshold`: Elimina caracter√≠sticas con baja varianza.
*  **`sklearn.compose`**: Herramientas para aplicar transformaciones a diferentes columnas.
    *   **`ColumnTransformer`:**  Aplica diferentes transformadores a diferentes columnas.
*   **`sklearn.dummy`:**  Estimadores "dummy" (para comparar con modelos reales).
    *   `DummyClassifier`, `DummyRegressor`.
* **`sklearn.calibration`**: Calibraci√≥n de probabilidades.
* **`sklearn.utils`**: Funciones de utilidad.